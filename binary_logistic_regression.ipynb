{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a853139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.metrics import classification_report, confusion_matrix  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6e8dc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>P100</th>\n",
       "      <th>P41</th>\n",
       "      <th>OspC</th>\n",
       "      <th>DbpA</th>\n",
       "      <th>BmpA</th>\n",
       "      <th>DbpB</th>\n",
       "      <th>P45</th>\n",
       "      <th>P58</th>\n",
       "      <th>P66</th>\n",
       "      <th>VlsE</th>\n",
       "      <th>ErpL</th>\n",
       "      <th>OspD</th>\n",
       "      <th>Diag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R49#02</td>\n",
       "      <td>1.983</td>\n",
       "      <td>2.176</td>\n",
       "      <td>5.882</td>\n",
       "      <td>1.739</td>\n",
       "      <td>1.791</td>\n",
       "      <td>3.951</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.939</td>\n",
       "      <td>2.021</td>\n",
       "      <td>12.456</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.784</td>\n",
       "      <td>Pos - Late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R49#03</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.921</td>\n",
       "      <td>0.824</td>\n",
       "      <td>8.279</td>\n",
       "      <td>5.255</td>\n",
       "      <td>3.925</td>\n",
       "      <td>2.138</td>\n",
       "      <td>9.256</td>\n",
       "      <td>2.860</td>\n",
       "      <td>9.868</td>\n",
       "      <td>1.633</td>\n",
       "      <td>1.640</td>\n",
       "      <td>Pos - Late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R49#04</td>\n",
       "      <td>1.304</td>\n",
       "      <td>2.654</td>\n",
       "      <td>1.484</td>\n",
       "      <td>11.073</td>\n",
       "      <td>2.456</td>\n",
       "      <td>9.750</td>\n",
       "      <td>1.851</td>\n",
       "      <td>3.670</td>\n",
       "      <td>3.945</td>\n",
       "      <td>11.615</td>\n",
       "      <td>2.026</td>\n",
       "      <td>1.569</td>\n",
       "      <td>Pos - Late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R49#11</td>\n",
       "      <td>0.951</td>\n",
       "      <td>1.917</td>\n",
       "      <td>1.394</td>\n",
       "      <td>1.197</td>\n",
       "      <td>1.415</td>\n",
       "      <td>2.359</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.321</td>\n",
       "      <td>1.372</td>\n",
       "      <td>4.370</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.309</td>\n",
       "      <td>Pos - Early Conv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R49#12</td>\n",
       "      <td>1.117</td>\n",
       "      <td>2.437</td>\n",
       "      <td>2.284</td>\n",
       "      <td>1.397</td>\n",
       "      <td>1.374</td>\n",
       "      <td>1.933</td>\n",
       "      <td>1.233</td>\n",
       "      <td>1.518</td>\n",
       "      <td>1.648</td>\n",
       "      <td>10.358</td>\n",
       "      <td>1.231</td>\n",
       "      <td>1.623</td>\n",
       "      <td>Pos - Early Conv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  P100   P41  OspC   DbpA  BmpA  DbpB   P45   P58   P66   VlsE  ErpL  \\\n",
       "0  R49#02 1.983 2.176 5.882  1.739 1.791 3.951 1.065 1.939 2.021 12.456 1.078   \n",
       "1  R49#03 1.000 2.921 0.824  8.279 5.255 3.925 2.138 9.256 2.860  9.868 1.633   \n",
       "2  R49#04 1.304 2.654 1.484 11.073 2.456 9.750 1.851 3.670 3.945 11.615 2.026   \n",
       "3  R49#11 0.951 1.917 1.394  1.197 1.415 2.359 1.012 1.321 1.372  4.370 0.999   \n",
       "4  R49#12 1.117 2.437 2.284  1.397 1.374 1.933 1.233 1.518 1.648 10.358 1.231   \n",
       "\n",
       "   OspD              Diag  \n",
       "0 1.784        Pos - Late  \n",
       "1 1.640        Pos - Late  \n",
       "2 1.569        Pos - Late  \n",
       "3 1.309  Pos - Early Conv  \n",
       "4 1.623  Pos - Early Conv  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('C:\\\\Users\\\\fd299212\\\\Desktop\\\\lab_Stuff\\\\collaborations\\\\cady\\\\machineLearning\\\\lyme_data_gcfp.txt',sep='\\t')\n",
    "df = pd.read_csv('C:\\\\Users\\\\fd299212\\\\Desktop\\\\lab_Stuff\\\\collaborations\\\\cady\\\\machineLearning\\\\lyme_data_20220520.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0457e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column in DF and populate with \"Neg\"\n",
    "#then alter to Pos for any 'Diag' column values that are not equal to \"Neg\" (various positive states)\n",
    "df['bin_diag'] = \"Neg\"\n",
    "df.loc[df['Diag']!=\"Neg\", 'bin_diag'] = \"Pos\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d94aa49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>P100</th>\n",
       "      <th>P41</th>\n",
       "      <th>OspC</th>\n",
       "      <th>DbpA</th>\n",
       "      <th>BmpA</th>\n",
       "      <th>DbpB</th>\n",
       "      <th>P45</th>\n",
       "      <th>P58</th>\n",
       "      <th>P66</th>\n",
       "      <th>VlsE</th>\n",
       "      <th>ErpL</th>\n",
       "      <th>OspD</th>\n",
       "      <th>Diag</th>\n",
       "      <th>bin_diag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R49#02</td>\n",
       "      <td>1.983</td>\n",
       "      <td>2.176</td>\n",
       "      <td>5.882</td>\n",
       "      <td>1.739</td>\n",
       "      <td>1.791</td>\n",
       "      <td>3.951</td>\n",
       "      <td>1.065</td>\n",
       "      <td>1.939</td>\n",
       "      <td>2.021</td>\n",
       "      <td>12.456</td>\n",
       "      <td>1.078</td>\n",
       "      <td>1.784</td>\n",
       "      <td>Pos - Late</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R49#03</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.921</td>\n",
       "      <td>0.824</td>\n",
       "      <td>8.279</td>\n",
       "      <td>5.255</td>\n",
       "      <td>3.925</td>\n",
       "      <td>2.138</td>\n",
       "      <td>9.256</td>\n",
       "      <td>2.860</td>\n",
       "      <td>9.868</td>\n",
       "      <td>1.633</td>\n",
       "      <td>1.640</td>\n",
       "      <td>Pos - Late</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R49#04</td>\n",
       "      <td>1.304</td>\n",
       "      <td>2.654</td>\n",
       "      <td>1.484</td>\n",
       "      <td>11.073</td>\n",
       "      <td>2.456</td>\n",
       "      <td>9.750</td>\n",
       "      <td>1.851</td>\n",
       "      <td>3.670</td>\n",
       "      <td>3.945</td>\n",
       "      <td>11.615</td>\n",
       "      <td>2.026</td>\n",
       "      <td>1.569</td>\n",
       "      <td>Pos - Late</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R49#11</td>\n",
       "      <td>0.951</td>\n",
       "      <td>1.917</td>\n",
       "      <td>1.394</td>\n",
       "      <td>1.197</td>\n",
       "      <td>1.415</td>\n",
       "      <td>2.359</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.321</td>\n",
       "      <td>1.372</td>\n",
       "      <td>4.370</td>\n",
       "      <td>0.999</td>\n",
       "      <td>1.309</td>\n",
       "      <td>Pos - Early Conv</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R49#12</td>\n",
       "      <td>1.117</td>\n",
       "      <td>2.437</td>\n",
       "      <td>2.284</td>\n",
       "      <td>1.397</td>\n",
       "      <td>1.374</td>\n",
       "      <td>1.933</td>\n",
       "      <td>1.233</td>\n",
       "      <td>1.518</td>\n",
       "      <td>1.648</td>\n",
       "      <td>10.358</td>\n",
       "      <td>1.231</td>\n",
       "      <td>1.623</td>\n",
       "      <td>Pos - Early Conv</td>\n",
       "      <td>Pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  P100   P41  OspC   DbpA  BmpA  DbpB   P45   P58   P66   VlsE  ErpL  \\\n",
       "0  R49#02 1.983 2.176 5.882  1.739 1.791 3.951 1.065 1.939 2.021 12.456 1.078   \n",
       "1  R49#03 1.000 2.921 0.824  8.279 5.255 3.925 2.138 9.256 2.860  9.868 1.633   \n",
       "2  R49#04 1.304 2.654 1.484 11.073 2.456 9.750 1.851 3.670 3.945 11.615 2.026   \n",
       "3  R49#11 0.951 1.917 1.394  1.197 1.415 2.359 1.012 1.321 1.372  4.370 0.999   \n",
       "4  R49#12 1.117 2.437 2.284  1.397 1.374 1.933 1.233 1.518 1.648 10.358 1.231   \n",
       "\n",
       "   OspD              Diag bin_diag  \n",
       "0 1.784        Pos - Late      Pos  \n",
       "1 1.640        Pos - Late      Pos  \n",
       "2 1.569        Pos - Late      Pos  \n",
       "3 1.309  Pos - Early Conv      Pos  \n",
       "4 1.623  Pos - Early Conv      Pos  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cdee2745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 15 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   ID        64 non-null     object \n",
      " 1   P100      64 non-null     float64\n",
      " 2   P41       64 non-null     float64\n",
      " 3   OspC      64 non-null     float64\n",
      " 4   DbpA      64 non-null     float64\n",
      " 5   BmpA      64 non-null     float64\n",
      " 6   DbpB      64 non-null     float64\n",
      " 7   P45       64 non-null     float64\n",
      " 8   P58       64 non-null     float64\n",
      " 9   P66       64 non-null     float64\n",
      " 10  VlsE      64 non-null     float64\n",
      " 11  ErpL      64 non-null     float64\n",
      " 12  OspD      64 non-null     float64\n",
      " 13  Diag      64 non-null     object \n",
      " 14  bin_diag  64 non-null     object \n",
      "dtypes: float64(12), object(3)\n",
      "memory usage: 7.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#df['Diag'].value_counts()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b4e072a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'P100', 'P41', 'OspC', 'DbpA', 'BmpA', 'DbpB', 'P45', 'P58',\n",
       "       'P66', 'VlsE', 'ErpL', 'OspD', 'Diag', 'bin_diag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b5697c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X = df.drop(['Diag','ID'], axis=1)  \n",
    "#X = df.filter(['VlsE', 'DbpA', 'P58', 'OspC','ErpL','DbpB'],axis=1)  \n",
    "X = df.filter(['VlsE', 'DbpA', 'P58', 'OspC','ErpL','P66'],axis=1)  \n",
    "\n",
    "#Data Standardization gives the data zero mean and unit variance, it is considered good practice, \n",
    "#especially for algorithms such as KNN which is based on the distance of data points\n",
    "#however, there is some disagreement about it for logistic regression...may require testing for specific dataset results\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\n",
    "X[0:5]\n",
    "y = df['bin_diag']\n",
    "#split original dataset into training and testing subsets\n",
    "#stratify=y ensures that the sampled sets attempt to represent each class's proportions as they were in the full set\n",
    "#the 'y' does not mean 'yes' it is the y vectors of class labels\n",
    "#note, random_state provides specific seed for pseudorandom generator to allow reproducible analysis of the model\n",
    "#remove this parameter to allow random selection each run\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1719e563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV] END ...C=0.1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=0.1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=0.1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=0.1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=0.1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .....C=0.1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=0.1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=0.1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=0.1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=0.1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=0.1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=1, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ...C=2.5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=2.5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=2.5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=2.5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=2.5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=2.5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=2.5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=2.5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=2.5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=2.5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .....C=2.5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=2.5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=2.5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=2.5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=2.5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=2.5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=2.5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=2.5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=2.5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=2.5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .......C=2.5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=2.5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=2.5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=2.5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=2.5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....C=5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=5, max_iter=8000, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .C=5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .C=5, max_iter=8000, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .......C=5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......C=5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=5, max_iter=8000, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=5, max_iter=8000, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=8000, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .........C=5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=5, max_iter=8000, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ....C=0.1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....C=0.1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....C=0.1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....C=0.1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....C=0.1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=0.1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END C=1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=1, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ....C=2.5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....C=2.5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ....C=2.5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....C=2.5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....C=2.5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=2.5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=2.5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=2.5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=2.5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=2.5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=2.5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=2.5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=2.5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=2.5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=2.5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END C=2.5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=2.5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=2.5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=2.5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=2.5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......C=5, max_iter=8000, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=5, max_iter=8000, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END C=5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=5, max_iter=8000, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=1, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=2.5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=5, max_iter=2000, penalty=l2, solver=liblinear; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1314, in fit\n",
      "    raise ValueError(\"l1_ratio must be between 0 and 1;\"\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.87777778 0.85555556 0.91777778 0.79555556 0.79555556 0.79555556\n",
      " 0.87777778 0.85555556 0.91777778 0.85555556 0.85555556 0.85555556\n",
      " 0.87777778 0.85555556 0.91777778 0.87555556 0.87555556 0.87555556\n",
      " 0.87777778 0.85555556 0.91777778 0.87555556 0.87555556 0.87555556\n",
      " 0.95777778 0.54222222 0.79555556        nan 0.95777778 0.81555556\n",
      " 0.85555556        nan 0.95777778 0.87555556 0.87555556        nan\n",
      " 0.95777778 0.91777778 0.87555556        nan 0.60222222 0.77333333\n",
      " 0.85555556 0.87555556 0.87555556 0.87555556 0.91777778 0.87555556]\n",
      "  warnings.warn(\n",
      "C:\\Users\\fd299212\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(),\n",
       "             param_grid=[{'C': [0.1, 1, 2.5, 5], 'max_iter': [8000],\n",
       "                          'penalty': ['none', 'l2'],\n",
       "                          'solver': ['lbfgs', 'newton-cg', 'sag']},\n",
       "                         {'C': [0.1, 1, 2.5, 5], 'max_iter': [8000],\n",
       "                          'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
       "                          'solver': ['saga']},\n",
       "                         {'C': [0.1, 1, 2.5, 5], 'max_iter': [2000],\n",
       "                          'penalty': ['l1', 'l2'], 'solver': ['liblinear']}],\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#the hyperparameters being fed to the gridsearch in this case may include some that are not \n",
    "#applicable with each other. Invocations with those may raise warnings that should be able to be ignored\n",
    "#but to the degree practicable, feed compatible parameters together...\n",
    "\n",
    "#we define the set of parameter values that will be passed in as \"param_grid\"\n",
    "#max_iter is set very high due to non convergence errors that had been occurring. This can be revisited as data set \n",
    "#continues to grow\n",
    "param_grid = [{'C': [.1,1,2.5,5], 'penalty': ['none','l2'],'solver': ['lbfgs','newton-cg', 'sag'], 'max_iter':[8000]},             \n",
    "             {'C': [.1,1,2.5,5], 'penalty': ['none','l1','l2','elasticnet'],'solver': ['saga'], 'max_iter':[8000]},\n",
    "              {'C': [.1,1,2.5,5], 'penalty': ['l1','l2'],'solver': ['liblinear'], 'max_iter':[2000]}]\n",
    "\n",
    "logreg=LogisticRegression()\n",
    "#this code implements the grid search\n",
    "grid = GridSearchCV(logreg,param_grid,refit=True,verbose=2)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aab3b51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_.penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06039a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 8000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'none', 'random_state': None, 'solver': 'saga', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "print(grid.best_estimator_.get_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d73b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Classes:\n",
      "['Pos' 'Pos' 'Neg' 'Pos' 'Neg' 'Neg' 'Pos' 'Pos' 'Neg' 'Pos' 'Pos' 'Neg'\n",
      " 'Pos' 'Neg' 'Pos' 'Neg']\n",
      "Actual Classes:\n",
      "26    Neg\n",
      "34    Pos\n",
      "54    Neg\n",
      "35    Pos\n",
      "31    Neg\n",
      "18    Neg\n",
      "19    Neg\n",
      "47    Pos\n",
      "50    Pos\n",
      "11    Pos\n",
      "49    Pos\n",
      "62    Neg\n",
      "38    Pos\n",
      "22    Neg\n",
      "51    Pos\n",
      "30    Neg\n",
      "Name: bin_diag, dtype: object\n",
      "Accuracy: 0.812 \n"
     ]
    }
   ],
   "source": [
    "model = grid.best_estimator_\n",
    "#following lines are not needed as gridsearchCV does 'refit' (retrains best estimator on full set provided[the whole\n",
    "#training set in this case]) by default\n",
    "#model.set_params(max_iter=5000)\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "yhat = model.predict(X_test)\n",
    "print('Predicted Classes:')\n",
    "print(yhat)\n",
    "print('Actual Classes:')\n",
    "print(y_test)\n",
    "\n",
    "score = model.score(X_test,y_test)\n",
    "# report the model performance\n",
    "print('Accuracy: %.3f ' % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16f36849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities:\n",
      "[[1.12900812e-01 8.87099188e-01]\n",
      " [4.04304777e-05 9.99959570e-01]\n",
      " [9.73793407e-01 2.62065930e-02]\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [9.97889870e-01 2.11013006e-03]\n",
      " [9.95991571e-01 4.00842876e-03]\n",
      " [4.38207965e-01 5.61792035e-01]\n",
      " [1.54004878e-03 9.98459951e-01]\n",
      " [6.55229487e-01 3.44770513e-01]\n",
      " [2.22044605e-16 1.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [9.98917053e-01 1.08294717e-03]\n",
      " [1.88643048e-03 9.98113570e-01]\n",
      " [9.74163553e-01 2.58364474e-02]\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [8.54056122e-01 1.45943878e-01]]\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities with a multinomial logistic regression model\n",
    "from sklearn.datasets import make_classification\n",
    "# predict a multinomial probability distribution\n",
    "yprobs = model.predict_proba(X_test)\n",
    "# summarize the predicted probabilities\n",
    "print('Predicted Probabilities:')\n",
    "print(yprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "db7c4f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>original sample index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.113</td>\n",
       "      <td>0.887</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.026</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998</td>\n",
       "      <td>0.002</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.996</td>\n",
       "      <td>0.004</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.562</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.998</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.655</td>\n",
       "      <td>0.345</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.001</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.998</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.974</td>\n",
       "      <td>0.026</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.854</td>\n",
       "      <td>0.146</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Negative  Positive  original sample index\n",
       "0      0.113     0.887                     26\n",
       "1      0.000     1.000                     34\n",
       "2      0.974     0.026                     54\n",
       "3      0.000     1.000                     35\n",
       "4      0.998     0.002                     31\n",
       "5      0.996     0.004                     18\n",
       "6      0.438     0.562                     19\n",
       "7      0.002     0.998                     47\n",
       "8      0.655     0.345                     50\n",
       "9      0.000     1.000                     11\n",
       "10     0.000     1.000                     49\n",
       "11     0.999     0.001                     62\n",
       "12     0.002     0.998                     38\n",
       "13     0.974     0.026                     22\n",
       "14     0.000     1.000                     51\n",
       "15     0.854     0.146                     30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#following is test code to output the probabilities in an easy to \n",
    "#read format using the dataframe display and format options\n",
    "import pandas as pd\n",
    "def plot_probabilities(prob_array, col_labels, sample_indices):\n",
    "    if yprobs.shape[1] == len(classes):\n",
    "        prob_df = pd.DataFrame(prob_array, columns=col_labels)\n",
    "        prob_df['original sample index'] = sample_indices\n",
    "        pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "        pd.set_option('display.precision', 3)\n",
    "        display(prob_df)\n",
    "    else:\n",
    "        print('Incorrect label list length')\n",
    "        \n",
    "classes = ['Negative','Positive']   \n",
    "rows = y_test.index\n",
    "print(\"Probabilities:\")\n",
    "plot_probabilities(yprobs , classes, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fbe4cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "#code from https://scikit-learn.org/0.18/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12cf9b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.86      0.75      0.80         8\n",
      "         Pos       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.81        16\n",
      "   macro avg       0.82      0.81      0.81        16\n",
      "weighted avg       0.82      0.81      0.81        16\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "[[6 2]\n",
      " [1 7]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, yhat)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print (classification_report(y_test, yhat))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Negative','Pos'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1eb30fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x263c92e3430>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 839.75x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "#this code also uses seaborn and produces a single pair plot of the VlsE and OspD columns of data, \n",
    "#and uses 'Diag' column categories to determine color per sample category\n",
    "sns.FacetGrid(df, hue='Diag', height=10).map(plt.scatter, 'VlsE', 'OspD').add_legend().set(\n",
    "    title='VlsE vs OspD Signals',\n",
    "    xlabel='VlsE',\n",
    "    ylabel='OspD'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6cb0ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.857 \n",
      "Run: 2\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 3\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 4\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 5\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 6\n",
      " Accuracy: 0.750 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.625 \n",
      "Run: 7\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 8\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 0.857 \n",
      "Run: 9\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.857 \n",
      "Run: 10\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.714 \n",
      "Run: 11\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 12\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 13\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.875 \n",
      "Run: 14\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 15\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.714 \n",
      "Run: 16\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.857 \n",
      "Run: 17\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.778 \n",
      " Specificity: 1.000 \n",
      "Run: 18\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 19\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 20\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.750 \n",
      "Run: 21\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 22\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 23\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.857 \n",
      "Run: 24\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 25\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 26\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 27\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.857 \n",
      "Run: 28\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 29\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.857 \n",
      "Run: 30\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.778 \n",
      " Specificity: 1.000 \n",
      "Run: 31\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.714 \n",
      "Run: 32\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 33\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.714 \n",
      "Run: 34\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 35\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.875 \n",
      "Run: 36\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 37\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.750 \n",
      "Run: 38\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 0.750 \n",
      " Specificity: 0.875 \n",
      "Run: 39\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 40\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 41\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 42\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.750 \n",
      "Run: 43\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.857 \n",
      "Run: 44\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 0.714 \n",
      "Run: 45\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.875 \n",
      "Run: 46\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 47\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 48\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 49\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.714 \n",
      "Run: 50\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 51\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 52\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 53\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 54\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.750 \n",
      "Run: 55\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 56\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 57\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 58\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 0.625 \n",
      " Specificity: 1.000 \n",
      "Run: 59\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 60\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.875 \n",
      "Run: 61\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 62\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.750 \n",
      " Specificity: 1.000 \n",
      "Run: 63\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.875 \n",
      "Run: 64\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.750 \n",
      "Run: 65\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.750 \n",
      " Specificity: 1.000 \n",
      "Run: 66\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 67\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 68\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.714 \n",
      "Run: 69\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 0.714 \n",
      "Run: 70\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.714 \n",
      "Run: 71\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 72\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 73\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 74\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 75\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.750 \n",
      "Run: 76\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.857 \n",
      "Run: 77\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.571 \n",
      "Run: 78\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.625 \n",
      "Run: 79\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.857 \n",
      "Run: 80\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.714 \n",
      "Run: 81\n",
      " Accuracy: 0.750 \n",
      " Sensitivity: 0.667 \n",
      " Specificity: 0.857 \n",
      "Run: 82\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 83\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 1.000 \n",
      "Run: 84\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 1.000 \n",
      "Run: 85\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 86\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 0.857 \n",
      "Run: 87\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 88\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.750 \n",
      "Run: 89\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 90\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.750 \n",
      "Run: 91\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 0.857 \n",
      "Run: 92\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 0.714 \n",
      "Run: 93\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.714 \n",
      "Run: 94\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 95\n",
      " Accuracy: 1.000 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 1.000 \n",
      "Run: 96\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 0.625 \n",
      " Specificity: 1.000 \n",
      "Run: 97\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.875 \n",
      " Specificity: 0.875 \n",
      "Run: 98\n",
      " Accuracy: 0.812 \n",
      " Sensitivity: 0.750 \n",
      " Specificity: 0.875 \n",
      "Run: 99\n",
      " Accuracy: 0.875 \n",
      " Sensitivity: 0.889 \n",
      " Specificity: 0.857 \n",
      "Run: 100\n",
      " Accuracy: 0.938 \n",
      " Sensitivity: 1.000 \n",
      " Specificity: 0.875 \n",
      "\n",
      "Average Accuracy: 0.911 \n",
      "Average Sensitivity: 0.927 \n",
      "Average Specificity: 0.891 \n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "accuracy_sum = 0\n",
    "sensitivity_sum = 0\n",
    "specificity_sum = 0\n",
    "\n",
    "count = 100\n",
    "if model.penalty == 'none':\n",
    "    new_model = LogisticRegression(penalty=model.penalty, solver=model.solver, max_iter=model.max_iter)\n",
    "else:\n",
    "    new_model = LogisticRegression(penalty=model.penalty, C=model.C, solver=model.solver, max_iter=model.max_iter)\n",
    "for n in range(1, count+1):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size = 0.25, stratify=y) \n",
    "\n",
    "    new_model.fit(X_tr, y_tr)\n",
    "    yh = new_model.predict(X_te)\n",
    "    cr = classification_report(y_te, yh, output_dict=True)\n",
    "    \n",
    "    curr_accuracy = metrics.accuracy_score(y_te,yh)\n",
    "    curr_sensitivity = cr[\"Pos\"][\"recall\"]\n",
    "    curr_specificity = cr[\"Neg\"][\"recall\"]\n",
    "    print(\"Run: %d\" % (n))\n",
    "    print(' Accuracy: %.3f ' % (curr_accuracy))\n",
    "    print(' Sensitivity: %.3f ' % (curr_sensitivity))\n",
    "    print(' Specificity: %.3f ' % (curr_specificity))\n",
    "    \n",
    "    sensitivity_sum = sensitivity_sum + curr_sensitivity\n",
    "    specificity_sum = specificity_sum + curr_specificity\n",
    "    accuracy_sum = accuracy_sum + curr_accuracy\n",
    "\n",
    "average_accuracy = accuracy_sum/count\n",
    "average_sensitivity = sensitivity_sum/count\n",
    "average_specificity = specificity_sum/count\n",
    "print('')\n",
    "print('Average Accuracy: %.3f ' % (average_accuracy))\n",
    "print('Average Sensitivity: %.3f ' % (average_sensitivity))\n",
    "print('Average Specificity: %.3f ' % (average_specificity))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d751bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff3d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
